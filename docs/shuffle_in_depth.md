Shuffle is the process of re-distribution of data between two partitions for the purpose of grouping together data with the same key value pair under one partition . This happens between two stages whenever a wide transformation or repartition is called
If we use MR paradigms , in the shuffle operation, the task that emits the data in the source executor is “mapper”, the task that consumes the data into the target executor is “reducer”, and what happens between them is “shuffle”.
Shuffle happens whenever there is a wide transformation or a repartition is called.
ShuffleManager is the authority that controls shuffle process . There were multiple implementations of shuffle prior to spark 2.0 and which implementation would be used in your particular case is determined by the value of spark.shuffle.manager parameter. Three possible options were there : hash, sort, tungsten-sort. But now we have SortShuffleManager is the only shuffle manager available with spark which implements shuffling based on all the three mentioned ways but gives priority to tungsten-sort (or sort).
 ShuffleManager is created in SparkEnv on the driver and on each executor based on the spark.shuffle.manager setting. (SparkEnv aka Spark runtime environment : which in turn created during sparkContext instantiation for driver JVM and after launch for each executors ) . The driver registers shuffles with it, and executors (or tasks running locally in the driver) can ask to read and write data. It is network-addressable, i.e. it is available on a host and port.
ShuffleManager is a pluggable interface which means , you can implement your own shuffle manager interface
 When the DAGScheduler generates the physical execution plan from our logical plan it pipelines together all RDDs into one stage that are connected by a narrow dependency. Consequently, it cuts the logical plan between all RDDs with wide dependencies. The final stage which produces the result is called the ResultStage. All other stages that are required for computing the ResultStage are called ShuffleMapStages.
The evaluation of a stage requires the evaluation of all partitions of the final RDD in that stage. Therefore, there will be one task for each of the partitions and, hence, the number of tasks of a stage is equal to the number of output partitions of the final RDD.
The tasks which are executed on a ShuffleMapStage is called ShuffleMapTask and tasks which are executed on a ResultStage is called ResultTask
Once we start evaluating the physical plan, Spark scans backwards through the stages and recursively schedules missing stages which are required to calculate the final result. As every stage needs to provide data to subsequent ones, the result of a ShuffleMapStage is to create an output file(aka shuffle file). This output file is partitioned by the partitioning function and is used by downstream stages to fetch the required data.
ShuffleMapTask on any executor performs below 3 operations:
Fetch : Reads the data from shuffle written files of previous stage by performing a shuffle read or reads data through a file scan from persistent storage (HDFS, S3,GCS,Azure Gen2 etc.)
Compute : Compute the output partition of next RDD in the DAG for the stage by applying the pipelined functions subsequently (The functions generated by CodeGen in the case of Spark SQL APIs: Dataframe/Dataset)
Write : Write the shuffle file containing shuffle partitions as blocks from the output partition it created above. This is done by requesting shuffle manager for a shuffle writer . All the difference in shuffle is implemented here by the shuffle writer provided by shuffle manager based on the shuffle strategy from the selected physical plan. A Partition function will be used based on the partitioner for partitioning the input partition into multiple shuffle partition blocks in the shuffle file it writes . Spark has two implementations of partitioners : HashPartitioner and RangePartitioner . Among these HashPartitioner is the most widely used one (For example Join strategies uses HashPartitioner ) . Also Partitioner is an abstract class (pluggable mechanism) to which you can implement your own Partitioners
 
Extra Note : Hash Partitioner decides the output partition based on hash code computed for key object specified for the data record as well as with the spark.sql.shuffle.partition value. (For example : if a shuffle key is 230 , then 230%spark.sql.shuffle.partition will decide the target partition ID. If spark.sql.shuffle.partition=200, then 230%200=30 will be the target partition ID. Hash Partitioner uses MurMur3 hashing algorithm )
Range Partitioner decides the output partition based on the comparison of key value against the range of key values estimated for each of the shuffled partition
 
At the time of write , ShuffleMapTask requests a ShuffleWriter from ShuffleManager. There are three implementations of shuffle writers in spark:BypassMergeSortShuffleWriter ,SortShuffleWriter,UnsafeShuffleWriter
 
BypassMergeSortShuffleWriter : 
SortShuffleManager's first choice when deciding shuffle strategy during physical planning.
This implements hash-based shuffle
This writer is selected by shuffle manager when there is no map-side combine (map-side aggregation) or number of shuffle partitions is less than spark.shuffle.sort.bypassMergeThreshold (default is 200 which is equal to default value of spark.sql.shuffle.partitions)
 
Note : Map side aggregation like reduceByKey,combineByKey mapSideCombine set to true,aggregateByKey,foldByKey uses map side aggregation and then reducer uses a combiner logic which sum up these aggregated values while groupByKey and combineByKey with mapSideCombine set to false won't use map side aggregation. Most Spark SQL aggregations uses map side aggregation
 
For ShuffleHashJoin , shuffle manager uses this writer
It produces 1 shuffle block file (aka shuffle file) per reducer (aka shuffle partition) and later it will be combined to for 1 filer per input partition. For example, if your dataframe is having 2 input partitions and spark.sql.shuffle.partition is 200 , 400 files will be written for a mapPartitionStage first. Then each 200 files are combined to 1 file each for each input partition. It also involves an index file each for each shuffle file with locations defined for each shuffle block inside a shuffle file 
Every shuffle file is serialized as Java objects. 
It is inefficient for shuffle partitions of large size and number since it opens a file and maintains a BufferedOutputStream for every partition, each requiring a memory buffer of the size of spark.shuffle.file.buffer (default 32k). Resulting in R FileStreams and memory allocated for buffers of the size R * bufferSize — where R is the number of reduce tasks (equivalent to the number of partitions after the shuffle). So on an executor with C cores there exist C * R open BufferedOutputStreams at a time. Furthermore, after writing the R files, we must re-access and read them (eventually into memory) to create an integrated output file.
SortShuffleWriter:
SortShuffleManager's third or last choice when deciding shuffle strategy during physical planning (yes , its 3rd , not a typo. Second is listed next because it is implemented based on the shuffling mechanism of this writer)
It is used if any of these conditions are met:
number of shuffle partitions is greater than spark.shuffle.sort.bypassMergeThreshold property
shuffle operation does involve a map-side aggregation 
shuffle serializer doesn't support object relocation
the number of shuffle partitions is greater than 16777216
This writer is the first implementation of sort-based shuffle. And UnsafeShuffleWriter implemented as an improvement to this which became selected as SortShuffleManager's second choice when deciding shuffle strategy during physical planning.
It try to write 1 shuffle file each for each input partitions by keeping all shuffle partitions computed by ShuffleMapTask for each input partition in memory . And finally records are written in order of shuffle partition id. 
If memory can't handle the complete map output , it will spill the data to disk . Shuffle spill is controlled by below spark properties:
spark.shuffle.spill is responsible for enabling/disabling spilling (I cannot see this property in spark 3.x versions , may be it is deprecated or removed since , disabling this is not a good idea)
spark.shuffle.compress - whether the engine would compress shuffle outputs or not. (Default is true)
spark.shuffle.spill.compress - whether to compress intermediate shuffle spill files or not. (Default is true)
spark.io.compression.codec - codec for compressing the data, which is snappy by default.
 
 The amount of memory that can be used for storing “map” outputs before spilling them to disk is :

(Java Heap (spark.executor.memory)— Reserved Memory) * spark.memory.fraction * (1.0 — spark.memory.storageFraction)

Calculation for spark.executor.memory=4GB with other values defaulted:

Execution Memory = (4096MB — 300MB) * 0.6 * (1.0 — 0.5) = ~1139MB
 
Above memory is what called as execution memory and is defined for whole executor which is shared by all tasks. So to get more memory share for your tasks , either increase `executor memory to number of cores ratio` or increase spark.cpu.tasks value which defines how many cores per task (default is 1. That's why we say 1 task will work on 1 core)

Note: 
execution memory + storage memory = spark memory which is calculated as  (spark.executor.memory— Reserved Memory)*spark.memory.fraction
SortShuffleManager uses JavaSerializer for writing shuffle data as java serialized objects. So if the data is spilled also , it follows this way of writing . And once all records of input partition is iterated , it re-access these spilled files by deserializing them again and merge them to form a single shuffle file per input partition which is ordered by shuffle partition id.
UnsafeShuffleWriter:
SortShuffleManager's second choice - An improvement to SortShuffleWriter.
This code is the part of project “Tungsten”. The idea is described here, and it is pretty interesting.
On a higher level implementation is same as SortShuffleWriter. But instead of writing as serialized java objects , it writes the shuffle data as serialized binary data which supports shuffle read without deserializing it . Its sort operates on serialized binary data rather than Java objects, which reduces memory consumption and GC overheads.
It uses a specialized cache-efficient sorter (UnsafeShuffleExternalSorter) that sorts arrays of compressed record pointers and partition ids.
The spill merging procedure operates on blocks of serialized records that belong to the same partition and does not need to deserialize records during the merge.
When the spill compression codec supports concatenation of compressed data, the spill merge simply concatenates the serialized and compressed spill partitions to produce the final output partition.
Spark SQL APIs (DataFrame , Dataset) uses UnsafeRowSerializer for writing the serialized binary data through this writer instead of spark's default spark.serializer which is JavaSerializer. KryoSerializer also supports this writer
This is selected when Selected when the number of shuffle partitions is greater than spark.shuffle.sort.bypassMergeThreshold and one of the following conditions is met:
shuffle serializer should support object relocation : object relocation is an important property of the UnsafeShuffleWriter because it stores serialized binary data instead of Java objects. Thus it reduces the memory consumption and GC overhead. And this serialized binary data can be read by shuffle reader without deserializing it. 
Note : BypassMergeSortShuffleWriter uses JavaSerializer which doesn't support object relocation since it writes data as serialized java objects (As java objects are created with references - an equivalent to a pointer , which points to all other objects to its actual data and methods. So you need to deserialize the java objects to get the data while shuffle read)
the processing code should not contain a map-side aggregation or output ordering
the number of shuffle partitions should be lower than 16777216
No individual record is larger than 128 MB when serialized.
 
Irrespective of shuffle writer , each shuffle block in an executor(a block corresponding to single shuffle partition ) is managed by the BlockManager. Every executor and the driver will have BlockManager created when spark runtime environment (sparkEnv) is instantiated . BlockManager created in driver sparkEnv is called BlockManagerMaster. Each block manager is having a unique id called BlockManagerID . BlockManager created on any executor registers with BlockManagerMaster through an RPC endpoint and shares the information such as executor id where it created, its host, port, the directory paths where it will be writing files etc.
Every shuffle block managed by the BlcokManager will have a unique ID called BlockId. This is a tuple of 3 identifications, ShuffleId, MapId and ReduceId. Here, ShuffleId uniquely identifies each shuffle write stage in a Spark application, MapId uniquely identifies each of the input partition(from which partition this is created) and ReduceId uniquely identifies each of the shuffled partition.
 
BlockManager uses BlockTransferService for fetching data from and uploading data to respective executors during shuffle read. This is implemented via ShuffleClient. This can be its bult-in shuffle service or an external shuffle service. External shuffle service will be used only if spark.shuffle.service.enabled is true. Built-in uses NettyBlockTransferService while external shuffle service can be any external shuffle implementations(There are many. Read this wonderful article for more info: https://medium.com/@rachit1arora/apache-spark-shuffle-service-there-are-more-than-one-options-c1a8e098230e). Spark's own Standalone external Shuffle Service uses ExternalShuffleClient as ShuffleClient. 
 
All shuffle blocks of a shuffle stage are tracked by MapOutPutTracker hosted in the driver. MapOutPutTracker also has master-slave implementation . MapOutPutTracker created on driver JVM is called MapOutPutTrackerMaster while on executor JVM is called MapOutPutTrackerWorker. Both are created with spark runtime instantiation and connected via RPC end point.
After a ShuffleMapTask has finished execution successfully, DAGScheduler is requested to handle a ShuffleMapTask completion status that in turn requests the MapOutputTrackerMaster to register the MapStatus . MapStatus contains all information about shuffle such as , BlockManagerId , list of blocks with blockid ,shuffle file location, shuffle file size etc. So MapOutputTrackerMaster can be called as the source of truth of shuffle map output locations. 
Now Task scheduler submit the next set of tasks in queue (as per decided by DAGScheduler). The RDD which need to be computed here is called shuffledRDD (in case of RDD API) or ShuffledRowRDD (in case of DataFrame or Dataset API). For computing partitions of this RDD , it has to resolve a ShuffleDependency. ShuffleDependency is a RDD Dependency on the output of a ShuffleMapStage on ShuffleRowRDD . For resolving ShuffleDependency , the task has to perform fetch operation which is called shuffle read.
Each task (aka reducer task/shuffle map task) needs to read corresponding block of data from shuffle map stage output shuffle file. This means reducer task with task ID 0 will read shuffle partition with partition ID 0
Please remember that , this executor also has a ShuffleManager (SortShuffleManager), BlockManager and MapOutputTracker running. ShuffleMapTask request ShuffleManager for a ShuffleReader. BlockStoreShuffleReader is the one and only shuffle reader available in spark
First SortShuffleManager identifies the location of the shuffle blocks to fetch from the MapOutputTracker and then it creates the BlockStoreShuffleReader with BlockManagerId, list of block ids, physical location of files etc. If the status of a Shuffle block is absent against a shuffle stage tracked by MapOutPutTrackerWorker , then it queries MapOutputTrackerMaster for it. If still it is absent, then it leads to ‘MetadataFetchFailedException’ in the reducer task corresponding to ReduceId in Shuffle block. The task will be failed and the dependent stage which should have written the shuffle files will be relaunched by DAGScheduler
Once BlockStoreShuffleReader is created , it initialize an iterator called ShuffleBlockFetcherIterator which is responsible for getting required block of data from respective shuffle file from the executor. It uses BlockManager's ExternalBlockStoreClient if shuffle service enabled, NettyBlockTransferService otherwise for fetching blocks.
Note: Failure in fetching the shuffle block from the designated Block manager leads to ‘FetchFailedException’ in the corresponding reducer task. And task will be failed and which should have written the shuffle files will be relaunched by DAGScheduler
 
Here comes the importance of external shuffle service. If we use built-in NettyBlockTransferService , if an executor is stopped due to some reason (for example if dynamic allocation is enabled and executor idle time is reached), it automatically removes generated files from the block manager written path. Now if NettyBlockTransferService of the other executor which is executing shuffle read task try to fetch the block from the location given by MapOutputTraker , it will fail with FetchFailedException. This will delay the whole process since now DAGScheduler needs to launch the stages again
 
External shuffle service is in fact a proxy through which Spark executors fetch the blocks. Thus, its lifecycle is independent on the lifecycle of executor. The service is created on a worker node and every time when it exists there, newly created executor registers to it. If we enable external shuffle service by setting spark.shuffle.service.enabled to true, BlockManager uses ExternalShuffleClient as ShuffleClient. Whenever a new BlockManager is created on a worker as a part of new executor , it register with this shuffler shuffle service and handover the directory locations where it will write the shuffle data. From here onwards ExternalShuffleService manages and monitor the shuffle output files for that executor. So even it goes down or getting killed , still the shuffle files will be available for read.
 
What if one worker node receives more data than any other worker? You will have to wait for that worker to finish processing while others do nothing. What’s even worse, the worker may fail, and the cluster will need to repeat a part of the computation. You can avoid such problems by enabling speculative execution - use the spark.speculation parameter. With this feature enabled, the idle workers calculate a copy of long-running tasks, and the cluster uses the results produced by the worker who finished sooner.
 
Checkout the Shuffle related configs here : https://spark.apache.org/docs/latest/configuration.html
 
Why dynamic allocation requires an external shuffle service
 
Dynamic allocation is a major method of allocating resources in an effective way which scales the number of executors registered with the application up and down based on the workload. This is enabled by setting spark.dynamicAllocation.enabled=true . But this also requires spark.shuffle.service.enabled or spark.dynamicAllocation.shuffleTracking set to true. And the most preferred option is setting spark.shuffle.service.enabled to true. Do you know why ??
 
How it works:
The spark application will be launched with an initial number of executors from which it can start execution of tasks.
Later according to the executor idle timeout defined , executors will be removed till the active or idle executors reach a minimum value . Similarly executors will be added according to the workload derived from pending tasks in queue.
 
 
These activities are done by ExecutorAllocationManager which is the core component of dynamic allocation service. ExecutorAllocationManager uses a ExecutorAllocationClient for requesting or killing tasks from cluster manager. ExecutorAllocationManager is instantiated when sparkContext is created .
 
When you enable dynamic allocation , some important properties that controls the behaviour are :
spark.dynamicAllocation.minExecutors : Minimum executors up to which ExecutorAllocationClient can kill the idle executor. Or in other words at least these many executors will be available as active throughout application lifecycle. Default is 0. 
spark.dynamicAllocation.maxExecutors : Maximum number of executors up to which ExecutorAllocationClient  can request. Or in other words these at most many number of executors can be found in a spark application at some point of time. Default is infinity.
spark.dynamicAllocation.initialExecutors: These are the initial executors which will be requested by ExecutorAllocationClient  first when we start an application. This cannot be less than minExecutors and greater than maxExecutors . If `--num-executors` (or `spark.executor.instances`) is set and larger than this value, it will be used as the initial number of executors. Default is spark.dynamicAllocation.minExecutors.
spark.dynamicAllocation.executorAllocationRatio : This ratio defines "actual allocation request to calculated need of executors" ratio for executors . ExecutorAllocationManager uses a performance metrics for calculation of required executors. This ratio defines how much of the calculated executors can be requested from cluster manager. By default , this is 1 which means , ExecutorAllocationClient  will request exactly the calculated value by ExecutorAllocationManager . But if we put less than 1 , say 0.5, ExecutorAllocationClient  will request only half of the executors calculated by ExecutorAllocationManager. This value should be between 0 and 1.
dynamicAllocation.executorIdleTimeout: This Defines the idle time after which an executor can be removed. Default is 60s.
spark.dynamicAllocation.shuffleTracking.enabled : Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. Default is false
spark.dynamicAllocation.shuffleTracking.timeout : Default time after which an executor which holds shuffle data can be removed if spark.dynamicAllocation.shuffleTracking.enabled is true. Default is infinity.
spark.shuffle.service.enabled : If spark.dynamicAllocation.shuffleTracking.enabled is not true , this should be enabled. Because otherwise , executors will be removed after the idle timeout (dynamicAllocation.executorIdleTimeout) and shuffle files will be lost. Default is false
From above properties we can see that , ExecutorAllocationClient will remove the idle executors once dynamicAllocation.executorIdleTimeout is reached. So if the executor removed holds some shuffle files ,then the BlockManager of that executor deletes the shuffle files produced by that executor from the worker node disk.
If we enable shuffle tracking by setting spark.dynamicAllocation.shuffleTracking.enabled as true, it will hold the executor from getting killed even after dynamicAllocation.executorIdleTimeout is reached. It can hold up to spark.dynamicAllocation.shuffleTracking.timeout which is 'infinity' by default. But this kills the purpose of dynamic allocation as this executor should be actually available for other tasks in queue (or should be released).
So the better option is always enabling the shuffle service. If we enable external shuffle service by setting spark.shuffle.service.enabled to true, BlockManager uses ExternalShuffleClient as ShuffleClient for managing shuffle files. This makes the tracking of shuffle files independent of executor's BlockManager. Shuffle files won't be removed even if executor is removed . And every time for a shuffle read , BlockManager of the reading executor uses ExternalShuffleClient of shuffle service for fetching blocks from the node where the other executor has written shuffle files.